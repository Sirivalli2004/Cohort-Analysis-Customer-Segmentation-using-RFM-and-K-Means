{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81573279",
   "metadata": {},
   "source": [
    "# Cohort Analysis & Customer Segmentation (RFM + K-Means)\n",
    "\n",
    "**Ready-to-run Jupyter Notebook**\n",
    "\n",
    "This notebook walks you step-by-step through:\n",
    "\n",
    "1. Loading and cleaning an e-commerce transactions dataset\n",
    "2. Computing RFM (Recency, Frequency, Monetary) scores\n",
    "3. Performing K-Means clustering on RFM features\n",
    "4. Performing Cohort Analysis and plotting a retention heatmap\n",
    "5. Visualizing segments and extracting business insights\n",
    "\n",
    "> **Note:** This notebook expects a dataset with columns similar to the `OnlineRetail.csv` dataset (InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country).\n",
    "\n",
    "If you don't have the dataset locally, you can download the **Online Retail** dataset from Kaggle or other public sources and place it in the same folder as this notebook with the name `OnlineRetail.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "Run cells in order. Explanations and comments are included inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c45573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed.\n",
    "# Uncomment and run if you need to install packages in your environment.\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn plotly openpyxl\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "print(\"Libraries imported. Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset. Replace the filename if yours is different.\n",
    "# Expected filename: 'OnlineRetail.csv'\n",
    "fname = 'OnlineRetail.csv'\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "    print(f\"File '{fname}' not found in the current directory.\")\n",
    "    print(\"Please download the Online Retail dataset (or your transactions CSV) and place it here with the name 'OnlineRetail.csv'.\")\n",
    "    print(\"You can also change the variable `fname` to the correct path.\")\n",
    "else:\n",
    "    df = pd.read_csv(fname, encoding='latin1')\n",
    "    print(\"Dataset loaded. Shape:\", df.shape)\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58be314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning and preprocessing\n",
    "# This block will run only if 'df' exists (i.e., dataset was loaded).\n",
    "\n",
    "if 'df' in globals():\n",
    "    # Show column names and dtypes\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(df.dtypes)\n",
    "\n",
    "    # Remove rows without CustomerID (we need customer-level analysis)\n",
    "    initial_len = len(df)\n",
    "    df = df.dropna(subset=['CustomerID'])\n",
    "    print(f\"Dropped rows without CustomerID: {initial_len - len(df)} rows removed. New shape: {df.shape}\")\n",
    "\n",
    "    # Remove canceled transactions (InvoiceNo starting with 'C' often marks cancellations)\n",
    "    if df['InvoiceNo'].dtype == object:\n",
    "        df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "\n",
    "    # Create TotalPrice column (Quantity * UnitPrice)\n",
    "    df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').fillna(0)\n",
    "    df['UnitPrice'] = pd.to_numeric(df['UnitPrice'], errors='coerce').fillna(0.0)\n",
    "    df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "    # Convert InvoiceDate to datetime\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "\n",
    "    # Drop rows with non-positive TotalPrice (optional)\n",
    "    df = df[df['TotalPrice'] > 0]\n",
    "\n",
    "    print(\"After cleaning, dataset shape:\", df.shape)\n",
    "    display(df[['InvoiceNo','InvoiceDate','CustomerID','Quantity','UnitPrice','TotalPrice']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82adde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RFM metrics\n",
    "if 'df' in globals():\n",
    "    # Latest order date (reference date for recency). Use a fixed date or dataset max date.\n",
    "    latest_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "    print(\"Reference date for Recency (latest_date):\", latest_date.date())\n",
    "\n",
    "    # Aggregate per customer\n",
    "    rfm = df.groupby('CustomerID').agg({\n",
    "        'InvoiceDate': lambda x: (latest_date - x.max()).days,  # Recency in days\n",
    "        'InvoiceNo': 'nunique',  # Frequency = number of orders/invoices\n",
    "        'TotalPrice': 'sum'  # Monetary = total spent\n",
    "    }).reset_index()\n",
    "\n",
    "    rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "    display(rfm.describe().T)\n",
    "    display(rfm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign RFM scores using quantiles (1-5)\n",
    "if 'rfm' in globals():\n",
    "    # We convert Recency so that lower recency (recent purchase) gets higher score.\n",
    "    r_labels = [5,4,3,2,1]  # 5 = most recent, 1 = least recent\n",
    "    rfm['R_Score'] = pd.qcut(rfm['Recency'], q=5, labels=r_labels).astype(int)\n",
    "\n",
    "    f_labels = [1,2,3,4,5]  # higher frequency => higher score\n",
    "    rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), q=5, labels=f_labels).astype(int)\n",
    "\n",
    "    m_labels = [1,2,3,4,5]  # higher monetary => higher score\n",
    "    rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=5, labels=m_labels).astype(int)\n",
    "\n",
    "    rfm['RFM_Score'] = rfm['R_Score']*100 + rfm['F_Score']*10 + rfm['M_Score']  # formatted score\n",
    "    rfm['RFM_Sum'] = rfm['R_Score'] + rfm['F_Score'] + rfm['M_Score']\n",
    "\n",
    "    display(rfm.head())\n",
    "    print(\"Value counts for RFM Sum:\")\n",
    "    display(rfm['RFM_Sum'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering on RFM numeric values\n",
    "if 'rfm' in globals():\n",
    "    # Prepare features\n",
    "    features = rfm[['Recency','Frequency','Monetary']].copy()\n",
    "\n",
    "    # It's recommended to log-transform Monetary to reduce skew\n",
    "    features['Monetary_log'] = np.log1p(features['Monetary'])\n",
    "    features_for_clustering = features[['Recency','Frequency','Monetary_log']]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(features_for_clustering)\n",
    "\n",
    "    # Choose number of clusters (k). Here we use k=4 as an example.\n",
    "    k = 4\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    rfm['Cluster'] = clusters\n",
    "    print(\"Cluster sizes:\")\n",
    "    display(rfm['Cluster'].value_counts().sort_index())\n",
    "\n",
    "    # Show cluster centers (inverse transform to original scale for interpretation)\n",
    "    centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    centers_df = pd.DataFrame(centers, columns=features_for_clustering.columns)\n",
    "    display(centers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters: mean RFM per cluster\n",
    "if 'rfm' in globals():\n",
    "    cluster_analysis = rfm.groupby('Cluster').agg({\n",
    "        'Recency':'mean',\n",
    "        'Frequency':'mean',\n",
    "        'Monetary':'mean',\n",
    "        'RFM_Sum':'mean',\n",
    "        'CustomerID':'count'\n",
    "    }).rename(columns={'CustomerID':'NumCustomers'}).reset_index()\n",
    "    display(cluster_analysis)\n",
    "\n",
    "    # Visualize clusters using scatter plot (Recency vs Monetary) with Frequency as size\n",
    "    fig = px.scatter(rfm, x='Recency', y='Monetary', color='Cluster', size='Frequency',\n",
    "                     title='Customer Segments: Recency vs Monetary (size=Frequency)',\n",
    "                     hover_data=['CustomerID','RFM_Sum'])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohort Analysis: calculate retention table\n",
    "if 'df' in globals():\n",
    "    # Create Month columns for order and cohort\n",
    "    df['OrderMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
    "    df['CohortMonth'] = df.groupby('CustomerID')['InvoiceDate'].transform('min').dt.to_period('M')\n",
    "\n",
    "    # CohortIndex: number of months since the cohort start\n",
    "    def get_month_diff(end, start):\n",
    "        return (end.year - start.year) * 12 + (end.month - start.month)\n",
    "\n",
    "    df['CohortIndex'] = df.apply(lambda row: get_month_diff(row['OrderMonth'].to_timestamp(), row['CohortMonth'].to_timestamp()), axis=1)\n",
    "\n",
    "    cohort_data = df.groupby(['CohortMonth','CohortIndex'])['CustomerID'].nunique().reset_index()\n",
    "    cohort_pivot = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
    "\n",
    "    # Cohort size (number of users in first month)\n",
    "    cohort_size = cohort_pivot.iloc[:,0]\n",
    "    retention = cohort_pivot.divide(cohort_size, axis=0)\n",
    "\n",
    "    # Display retention heatmap\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(retention, annot=True, fmt='.0%', cmap='Blues')\n",
    "    plt.title('Cohort Retention Table')\n",
    "    plt.ylabel('Cohort Month')\n",
    "    plt.xlabel('Months Since Cohort')\n",
    "    plt.show()\n",
    "\n",
    "    display(retention.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cf219",
   "metadata": {},
   "source": [
    "## Business Insights & Next Steps\n",
    "\n",
    "- **Interpret clusters**: Label clusters as \"High-Value\", \"At-Risk\", \"New\", \"Low-Value\" based on means in `cluster_analysis`.\n",
    "- **Actions**:\n",
    "  - High-value: loyalty programs, exclusive offers.\n",
    "  - At-risk: re-engagement campaigns, special discounts.\n",
    "  - New: welcome series and onboarding.\n",
    "  - Low-value: evaluate cost-to-serve; tailor offerings.\n",
    "- **Next steps for production**:\n",
    "  - Store RFM scores and cluster IDs in a database for downstream use.\n",
    "  - Build a dashboard (Power BI / Tableau / Streamlit) to track cohort retention and segment distribution.\n",
    "  - Consider predictive models (churn prediction) using time-series or classification models.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can:\n",
    "1. Generate a downloadable copy of this notebook.\n",
    "2. Create a small sample CSV to test the notebook if you don't have the real dataset.\n",
    "3. Convert key plots into static PNGs for embedding in a report."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
